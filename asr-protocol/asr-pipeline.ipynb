{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topiary\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project organization\n",
    "\n",
    "\n",
    "Keeping track of all of the files is always annoying in these projects.  I'm not sure there is a \"right\" way to do it, but here are a few principles that have kept me sane over the years:\n",
    "\n",
    "+ Whenever you have to save out a file, do it with a numbered prefix and description.  Something like `00_initial-sequence-to-blast.fasta`, `01_blast-results.xml`, etc. This way, if you sort the directory, you can see what you did, in what order. \n",
    "+ Keep your sequences in a `.csv` file with columns for species, database id, raw sequence, aligned sequence, etc. You can then share this `.csv` file as the supplement to your paper.  *This protocol implements this `.csv` strategy throughout.*\n",
    "\n",
    "PARADIGM\n",
    "```\n",
    "df = do_something(df)\n",
    "topiary.write_dataframe(df,\"dataset.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Create a set of BLAST .xml files containing sequences\n",
    "\n",
    "Building sequence datasets is complicated.  How you go about it depends on what your goals are for the project.  In this protocol, I am going to assume we are looking members of protein families (meaning a mixture of orthologs and paralogs) from animals (meaning minimal lateral gene transfer). \n",
    "\n",
    "I'll mention two questions that often come up.  The first is: *should I include paralogs and orthologs?* For a gene tree—what we normally generate—you generally want to a large number of orthologs and paralogs. A good check for the quality of the tree is whether the orthologs group together and roughly reproduce the species tree.  In practice, this means BLASTing away and then grabbing as many sequences as possible without worrying too much about whether you are pulling down orthologs or paralogs.  \n",
    "\n",
    "One reason this is useful is that sequence databases sometimes have paralogs and orthologs mislabeled.  Imagine you are building a tree of protein *X*.  You BLAST the NCBI and pull out out a sequence labeled protein *Y*.  You then build a tree, including this protein, and find it groups squarely with the *X* proteins, but not the other *Y* proteins in your dataset.  The simplest explanation for this result is that the protein was annotated incorrectly.   If you had dropped this sequence from your analysis, just because it was labeled *Y*, you would have removed valuable information from your dataset.  \n",
    "\n",
    "A second question: *should I include \"hypothetical,\" predicted\", and \"low-quality\" sequences?* As is usual for bioinformatics, the answer is \"it depends.\" If the protein has a huge number of isoforms and weirdo splice sites, the predicted genes could be difficult to align and thus mess up your phylogeny. On the other hand, if the protein has a very well defined set of exons, you'd probably be okay. Further, you sometimes really need sequences from taxa for which sequencing data are particularly poor and a \"low-quality\" sequence is the best you can do.  In the following pipeline, we include everything up front, and then remove redundant sequences.  We preferentially remove these low-quality sequences if better sequences are available. \n",
    "### Practical thoughts:\n",
    "\n",
    "+ The bigger the initial dataset, the better. We can pare down later. \n",
    "\n",
    "+ Do not worry about duplicate sequences at this point; we'll remove them later. \n",
    "\n",
    "+ BLAST with multiple paralogs for the gene of interest, sampled from multiple species. \n",
    "\n",
    "+ Use PSI-BLAST to iteratively build your dataset.  In PSI-BLAST, your results from the first BLAST search are fed into the next BLAST search, yielding more hits overall.  This process can be repeated until you find no more new hits. \n",
    "\n",
    "+ Check for good taxonomic sampling. (You gain more information from 1,000 sequences from across mammals than just taking 1,000 sequences from rodents.) The `Taxonomy` tab on the blast interface is useful for this. If you are not seeing sequences from key species, you can do targeted BLAST within a clade.  \n",
    "\n",
    "  + To do this, use the `Organism` selection flag to make sure you get sequences from key lineages.  Since we often study vertebrate proteins, here's a useful strategy: do separate BLAST searches for: `Mammalia (taxid:40674)`, `Sauropsida (taxid:8457)`, `Amphibia (taxid:8292)`,  `bony fishes  (taxid:7898)`,  `Elasmobranchii (taxid:7778)` (skates, rays and sharks), `jawless vertebrates (taxid:1476529)` (hagfish and lampreys), and `tunicates (taxid:7712)` (closest non-vertebrate outgroup). I also usually do `lobe-finned fishes (taxid:118072)`, excluding `Tetrapoda (taxid:32523)`.  This should pull up lungfish and coelacanth sequences if available. \n",
    "  + If you get a ton of hits in your earliest-diverging lineage--`tunicates` above--it suggests the protein evolved earlier than you have sampled.  If so, expand to earlier-diverging groups.  In this case, you would expand to earlier-diverging groups like `Ecdysozoa (taxid:1206794)` (which includes both *D. Melanogaster* and *C. elegans*), `Lophotrochozoa (taxid:1206795)` and `Hemichordata (taxid:10219)`. \n",
    "  + If one of those searches yields a small number of hits, it might be worthwhile to search for non-NCBI genomes and transcriptomes in an effort to fill out the taxa. Some lineages are just have few sequence resources.  Amphibians are notoriously undersampled, and there are very few extant jawless vertebrates. As a result, these bits of vertebrate protein/gene treees are often sparse. \n",
    "  \n",
    "### Other sources of sequences\n",
    "\n",
    "*Note: If you have to go this route, you'll have to manually load the non-NCBI results into an existing topiary dataframe in excel or the like. See #12 XX below for an example. After manually adding information to the .csv file, you have to re-load it into a pandas dataframe and then write it back out into a new .csv file. Maybe this puts everything into the right dtype (?)\n",
    "*\n",
    "\n",
    "+ Some good places to look for sequences outside of NCBI are [Fish10K](https://db.cngb.org/datamart/animal/DATAani16/) and [Bird10K](https://b10k.genomics.cn/index.html). You can also switch to [tblastn](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=tblastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome) approach to see if there are nucleic acid sequences corresponding to your protein that have not been annotated as proteins.  I find [https://ensembl.org/](https://ensembl.org/) is the easiest database for this task. Finally, if you are desperate, can look for transcriptomes in the [short read archive](https://www.ncbi.nlm.nih.gov/sra).  These often have to be assembled (a somewhat tedious process). \n",
    "\n",
    "+ [PFAM](http://pfam.xfam.org/) has some amazing pre-built alignments.  I've generally found them more useful for studies of whole domains evolving over very long timescales than our typical vertebrate protein work, but it's worth keeping in mind these alignments are out there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sequences from xml files into a dataframe\n",
    "\n",
    "This will create a data frame and .csv file with all of the sequences you identified by BLAST.  Most of the columns are self-explanatory, but there are two important columns that bear more explanation: \n",
    " + `uid`: a random 10-letter string unique to each sequence, used for generating files compatible with PAML. \n",
    " + `keep`: whether or not the sequence should be written out in alignments. Sequences are never deleted from the dataframe, just marked as `keep = False`. \n",
    " \n",
    "Running this code requires you provide a list of `.xml` files.  You can also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of xml files to load and process\n",
    "list_of_xml_files = [\"../data/mammalian.xml\"] #,\"example/sauropsid.xml\"]\n",
    "\n",
    "# Load sequences into data frame\n",
    "df = topiary.ncbi_blast_xml_to_df(list_of_xml_files) #,aliases=alias_dictionary)\n",
    "\n",
    "# Write output file\n",
    "topiary.write_dataframe(df,\"01_initial-hits.csv\")\n",
    "\n",
    "# Print to notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign nicknames to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliases as a dictionary. It will map anything in the values to the key. For\n",
    "# this example, \"lymphocyte antigen 96\", \"MD2\", and \"MD-2\" will all be replaced\n",
    "# by \"LY96\". \n",
    "alias_dictionary = {\"LY96\":(\"lymphocyte antigen 96\",\"MD2\",\"MD-2\"),\n",
    "                    \"LY86\":(\"lymphocyte antigen 86\",\"MD1\",\"MD-1\")}\n",
    "\n",
    "df = topiary.create_nicknames(df,aliases=alias_dictionary)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find unique species identifiers\n",
    "\n",
    "Idea here is to make sure we actually know what species each sequence comes from. NCBI and the opentree of life use slightly different sequence names. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = topiary.get_ott_id(df,phylo_context=\"Animals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THINK WE SHOULD HAVE SOMETHING LIKE THIS FOR MR/GR?\n",
    "\n",
    "In these example data, \"Apteryx mantelli mantelli\" is not found in opentree of life.  If you start typing the species name into the open tree of life search engine (https://tree.opentreeoflife.org/), it pops up \"Apteryx australis mantelli\".  A quick google search reveals these are the same species. You can update this by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df.species == \"Apteryx mantelli mantelli\",\"species\"] = \"Apteryx australis mantelli\"\n",
    "df.loc[df.species == \"Apteryx australis mantelli\",\"keep\"] = True\n",
    "df = topiary.get_ott_id(df,phylo_context=\"Animals\")\n",
    "df[df.ott.isnull()]\n",
    "\n",
    "topiary.write_dataframe(df,\"02_with-ott.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sequence identities using reverse BLAST\n",
    "\n",
    "BLAST can pull down sequences that are homologous, but outside the clade of interest.  (For example, we might want to study TLR4, but BLAST also pulls up TLR2). To identify these sequences, we use reverse BLAST each sequence against the human genome. We will keep only those sequences that pull up the proteins of interest from the human genome. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = topiary.read_dataframe(\"01_initial-hits.csv\")\n",
    "\n",
    "# Perform reverse blast, looking for hits on \"lymphocyte antigen 96\" and \n",
    "# \"lymphocyte antigen 86\" from the human genome, labeling them as LY96 \n",
    "# and LY86 respectively. \n",
    "\n",
    "# Command to blast against NCBI nr database, selecting only human. To search\n",
    "# based on more one taxid, pass a list of taxid\n",
    "# df = topiary.reverse_blast(df,\n",
    "#                            call_dict={\"LY96\":[\"lymphocyte antigen 96\"],\n",
    "#                                       \"LY86\":[\"lymphocyte antigen 86\"]},\n",
    "#                            ncbi_rev_blast_db=\"nr\",taxid=9606)\n",
    "\n",
    "# Command to blast against a local database named GRCh38\n",
    "df = topiary.reverse_blast(df,\n",
    "                          call_dict={\"LY96\":[\"lymphocyte antigen 96\"],\n",
    "                                     \"LY86\":[\"lymphocyte antigen 86\"]},\n",
    "                          local_rev_blast_db=\"GRCh38\")\n",
    "\n",
    "# Write output file\n",
    "topiary.write_dataframe(df,\"03_reverse-blasted.csv\")\n",
    "\n",
    "# Print to notebook\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lower redundancy of sequences\n",
    "\n",
    "To make the computation faster and avoid bias from inclusion of many very similar sequences, we usually remove sequences that are highly similar to one another. The code below will combine sequences with identities greater than 0.9, using relatively intelligent criteria to choose the higher quality sequence. \n",
    "\n",
    "The `key_species` list is a list of species that will be given preference over others. If we specify `[\"Homo sapiens\",\"Mus musculus\"]` as key species and then find a human and chimp sequence are highly similar, the software will drop the chimp. The software does two loops.  In the first loop, it discards similar sequences within each species. This *will* drop sequences from key species. (If you had two human sequences 99% identical, it would drop the lower quality sequence).  In the second loop, the software discards similar sequences between species.  That loop *will not* drop sequences from key species.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = topiary.read_dataframe(\"03_reverse-blasted.csv\")\n",
    "\n",
    "# Remove redundancy\n",
    "key_species = [\"Homo sapiens\",\"Mus musculus\",\"Monodelphis domestica\",\"Gallus gallus\",\n",
    "               \"Xenopus laevis\",\"Danio rerio\"] #,\"Tachyglossus aculeatus\", \"Ornithorhynchus anatinus\"\n",
    "df = topiary.remove_redundancy(df,0.90,key_species=key_species)\n",
    "\n",
    "# Write out file\n",
    "topiary.write_dataframe(df,\"04_removed-redundancy.csv\")\n",
    "\n",
    "# Print in notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check and edit reduced data frame\n",
    "\n",
    "At this point, you might want to look over the set of sequences and see if you like the result of the automatic redundancy reduction.  Some things to look for:\n",
    "\n",
    "+ Is your sequence set still huge (>1000) or too small (<100)? (If so, you might want to play with the redundnacy cutoff above). \n",
    "+ Do you still have the sequence of modern species you care about? (If so, you may want to manually set those sequences to `keep = True`). \n",
    "\n",
    "You could load up the nonredudnant set in excel, but I *strongly* recommend you do these manipulations using the pandas dataframe. Pandas slicing allows you to easily pull out rows you care about based on selection criteria. The cell below shows a few of these slicing approaches as templates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many sequences are there in the dataset?\n",
    "df = topiary.read_dataframe(\"04_removed-redundancy.csv\")\n",
    "print(np.sum(df.keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each of the key species (defined above) are in the dataset?\n",
    "key_species = [\"Homo sapiens\",\"Mus musculus\",\"Monodelphis domestica\",\"Gallus gallus\",\n",
    "               \"Xenopus laevis\",\"Danio rerio\"] #,\"Tachyglossus aculeatus\", \"Ornithorhynchus anatinus\"\n",
    "\n",
    "for k in key_species:\n",
    "    print(k,np.sum(np.logical_and(df.keep,df.species==k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all of the human sequences we're keeping\n",
    "df[np.logical_and(df.keep,df.species==\"Homo sapiens\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show all of the Danio rerio sequences we started with\n",
    "df1 = topiary.read_dataframe(\"01_initial-hits.csv\")\n",
    "df1 = df1.loc[df1.species == \"Danio rerio\",:]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align the sequences using MUSCLE\n",
    "\n",
    "We now have a database of sequences.  We now need to align those sequences to one another.  The code below will create a file called `04_to-align.fasta` that has all of the sequences flagged with `keep = True`. The sequences will be assigned \"pretty\" names that have a defined structure: `ortholog_call|species|accession`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = topiary.read_dataframe(\"04_removed-redundancy.csv\")\n",
    "\n",
    "# Write fasta file. \n",
    "topiary.write_fasta(df,\"05_to-align.fasta\")\n",
    "\n",
    "\n",
    "topiary.run_muscle(input_fasta=\"05_to-align.fasta\",\n",
    "                   output_fasta=\"06_aligned.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually edit the alignment using AliView\n",
    "\n",
    "XX NOTE THIS IS ASR, SO SLIGHTLY DIFFERENT THAN OTHER TREE INFERENCE PROBLEMS XX\n",
    "Human brains are still better than computers at identifying patterns in sequence data. We're going to edit the alignment manually. Two consequences of this:\n",
    "\n",
    "1. We need to publish our final alignment with our manuscript. It needs to be available for evaluation by readers and/or to reproduce the work. \n",
    "\n",
    "2.  If we're doing ancestral sequence reconstruction an we *delete* a column, we need to make sure we're okay with not including that column in the final reconstruction.  This probably makes sense for N- and C-terminal extensions, but makes less sense for columns in the middle of the protein. \n",
    "\n",
    "With that in mind, open the `05_aligned.fasta` file in aliview and do the following:\n",
    "\n",
    "1. <span style=\"color:blue\">*Look for sequences that are way longer or shorter than the average.*</span>  Super long sequences may align poorly--and suck other sequences into that poor alignment.  Super short sequences are also difficult to align and provide little taxonomic information.  Delete these sequences by selecting them and going to `Edit->Delete selected`.  When you reimport the alignment into your dataframe, the software will set `keep = False` for any sequence you deleted. \n",
    "\n",
    "2. <span style=\"color:blue\">*Trim random long N-terminal and C-terminal extensions from alignment.*</span>  Deleting sequences that align poorly will not bias your tree, but including incorrectly aligned regions might. Select these sequence regions and go to `Edit->Clear selected bases`. \n",
    "\n",
    "3. <span style=\"color:blue\">*Manually realign problematic regions.*</span>  Sometimes, alignment programs will make obvious mistakes, where the same sequence element is aligned different ways right next to one another.  To correct for this, you can manually move groups of amino acids by selecting them and dragging them right or left. You can also select whole blocks of the alignment and then go to `Align->Realign selected block` to re-run MUSCLE on that block. \n",
    "\n",
    "4. <span style=\"color:blue\">*Remove empty columns and rows.*</span> Remove gaps-only columns (`Edit->Delete gap-only columns`) and any empty sequences (`Edit->Delete empty sequences`). \n",
    "\n",
    "5. <span style=\"color:blue\">*Save out the edited alignment*</span> as:\n",
    "```\n",
    "06_aligned-edited.fasta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load newly aligned sequences into our dataframe and write out .csv file for tree building.\n",
    "\n",
    "We will now load our alignment back into our dataframe.  This will create a new column called \"alignment\" with the aligned sequences and will also set all sequences *not* in the alignment file to have \"keep = False\".  We will then write out the aligned sequences into a new .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = topiary.read_dataframe(\"04_removed-redundancy.csv\")\n",
    "\n",
    "# Load the alignment into the data frame\n",
    "df = topiary.read_fasta(df,\"06_aligned.fasta\",load_into_column=\"alignment\")\n",
    "\n",
    "# Write out file\n",
    "topiary.write_dataframe(df,\"07_seq-database.csv\")\n",
    "\n",
    "# Print df to notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFTER HERE NOT UPDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## KO note -  \n",
    "## If you make manual changes to your .csv file after this step, for example if you\n",
    "## decide to add some sequences from taxa that were not found in your ncbi search \n",
    "## and then add those sequences by hand into your .csv file, it seems to cause issues.\n",
    "## I found that my issues were remedied if I re-loaded my .csv into a pandas dataframe \n",
    "## and then wrote it back out into a new .csv file. Maybe this puts everything into the \n",
    "## right dtype (?)\n",
    "\n",
    "# Read in the .csv file that has been manually edited\n",
    "df = topiary.read_dataframe(\"07.1_seq-database.csv\")\n",
    "\n",
    "# Load the alignment into the data frame\n",
    "df = topiary.load_fasta(df,\"06_aligned-edited.fasta\",load_into_column=\"alignment\")\n",
    "\n",
    "# Write out file\n",
    "topiary.write_dataframe(df,\"07.2_seq-database.csv\")\n",
    "\n",
    "# Print df to notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. Generate ML phylogenetic tree using RAxML\n",
    "\n",
    "Our next steps are to find a good evolutionary model that describes our data and to build a maximum likelihood phylogenetic tree.  This is likely something you will want to run on a high-performance computing cluster.  \n",
    "\n",
    "You need to copy \"07_seq-database.csv\" (or your most current version) and `run_raxml.srun` (from Harms Lab GitHub asr-protocol/template/copy-to-hpc/) to a working directory in whatever server you use. *Hopefully it already has \"raxmlHPC\" installed - KO note - what does this mean?.  \n",
    "\n",
    "You then need to execute two commands, using the helper `run_raxml.srun` script. See below:\n",
    "\n",
    "### This is what your run_raxml.srun file should look like.\n",
    "\n",
    "    #!/bin/bash -l\n",
    "    #SBATCH --account=harmslab      ### change this to your actual account for charging\n",
    "    #SBATCH --job-name=raxml        ### job name\n",
    "    #SBATCH --output=hostname.out   ### file in which to store job stdout\n",
    "    #SBATCH --error=hostname.err    ### file in which to store job stderr\n",
    "    #SBATCH --partition=long        ### can be short long fat longfat\n",
    "    #SBATCH --time=07-00:00:00      ### Run for 7 days\n",
    "    #SBATCH --nodes=1               ### Run on a single node\n",
    "    #SBATCH --ntasks-per-node=1     ### Run one job on the node\n",
    "    #SBATCH --cpus-per-task=28      ### Use 28 cores to run job (should match threads below)\n",
    "\n",
    "    module load gcc\n",
    "\n",
    "    # Find the best phylogenetic model\n",
    "    run-raxml model -c 07_seq-database.csv -o find-model -T 28\n",
    "\n",
    "    # Consruct the ML tree\n",
    "    run-raxml ml -c 07_seq-database.csv -m `cat find-model/best-model.txt` -o ml-tree -T 28\n",
    "\n",
    "    # Construct ancestors on the ML tree.\n",
    "    run-raxml anc -c 07_seq-database.csv -m `cat find-model/best-model.txt` -t ml-tree/02_ml-tree.newick -o ml-anc -T 28\n",
    "\n",
    "    # Construct ancestors on the reconciled generax tree with supports\n",
    "    # run-raxml anc -c topiary.csv -m `cat find-model/best-model.txt` -t generax-run/reconciled-tree.newick -o ml-anc_reconciled --anc-support-tree generax-run/reconciled-tree-with-supports.newick -T 28\n",
    "\n",
    "#### What does this script say?\n",
    "The computing cluster at University of Oregon uses SBATCH commands. Change to your clusters preferences.\n",
    "\n",
    "The Environment Modules package is a tool that simplifies shell initialization and lets users easily modify their environment during a session using modulefiles. \n",
    "```\n",
    "module load gcc\n",
    "```\n",
    "\n",
    "The first will search through a collection of different models of evolutionary rate distribution, amino acid substitution probability, etc. and find the model that gives the highest likelihood.  \n",
    "\n",
    "```\n",
    "run-raxml model -c 07_seq-database.csv -o find-model -T 28\n",
    "```\n",
    "\n",
    "This will print out the best model at the end, along with an `AIC Prob` score.  Hopefully, that number is close to 1.0, meaning that the chosen model is clearly a better choice than all the others. (If not, we may need to reconstruct our ancestors using different models to make sure the results are robust to the choice of model).  If you want to see the likelihoods and AIC probabilities for all models, check out `find_model/model-comparison.csv`). \n",
    "\n",
    "We next need to build the maximum likelihood phylogenetic tree for our alignment. To do that, run the following.  It will automatically grab the best model from the previous calculation. \n",
    "```\n",
    "run-raxml ml -c 07_seq-database.csv -m `cat find-model/best-model.txt` -o ml-tree -T 28\n",
    "```\n",
    "\n",
    "Constructing ancestors on the ML tree is not necessary at this step, but you can delete the comment here and have it run if you would like to see these ancestors. We cannot yet construct ancestors on the reconciled generax tree with supports, so leave this commented out.\n",
    "\n",
    "Once complete, you can download the resulting `find-model/` and `ml-tree/` directories to your local computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 11. Evaluate tree\n",
    "\n",
    "The next step is to look at the ML tree and make sure it is well supported/sensical. To do so, you should first write out the tree with useful names for each taxon. To do so, run the following.\n",
    "\n",
    "KO note: When trying to open my ML tree in FigTree I got an error saying there was a missing close parentheses. Turns out I had some unwanted punctuation in some of my manually entered accession numbers. This was causing problems with FigTree interpretting where to arrange species on a tree. Updated step 2 cell addresses this early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"07_seq-database.csv\")\n",
    "\n",
    "# Make the sequence names on the output tree human readable. \n",
    "topiary.util.uid_to_pretty(df,\"ml-tree/02_ml-tree.newick\",out_file=\"08_ml-tree.newick\")\n",
    "                           #\"ml-tree/07_final-tree.newick\",out_file=\"11_ml-tree.newick\")\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `08_ml-tree.newick` in FigTree.  On the left-hand panel, go to \"Branch-labels\" and select \"Display: label\". This will label each branch with its SH support.  SH support values go from 0 (no support at all) to 100 (excellent support).  Then look at the following:\n",
    "\n",
    "+ *Are the major clades well supported?* Major branch points should (hopefully) have $SH \\ge 85$. If not, we may need to do our reconstructions on multiple versions of the tree to see if our ancestral sequences are robust to the tree topology. \n",
    "+ *Is the species tree approximately correct?* Do you see birds with birds, mammals with mammals, etc.?   If not, there could be a problem with the current alignment, or we might need to add more sequences to the alignment. \n",
    "+ *Are there long branches?* A long branch is one where you have a bunch of sequence change (say 0.7 subs/site) without branching.  This means the evolutionary model runs and runs without getting input from branches.  This can lead to bias and will certainly lead to very poor reconstructions of ancestors near the long branch. If there is a long branch for a single sequence, delete it from the alignment. It is too divergent or too poorly aligned to include effectively. If a long branch occurs between clades, you can try to find new sequences that \"break\" the branch.  For example, if there is a long branch between bony fishes and birds, adding amphibian sequences will cut the branch (about) in half and should improve the inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Iteratively add sequences to alignment\n",
    "\n",
    "At this point, you may need to go back and add sequences to the alignment. To do so, you have a couple of options.  One possibility is to open `07_seq-database.csv` in excel and manually add any new sequences to the database. Make sure you fill out every column, including pasting the sequence into the `alignment` column. You could also add new rows via pandas (see #5 for some examples). \n",
    "\n",
    "After you've edited the sequence database, save it out as `09_seq-database.csv`. Write this out as a fasta file.  You can then load into aliview, edit, and repeat steps 6-9 until you are satisfied with the ML tree.  (The following code block is an example of what you might run in a jupyter notebook.) \n",
    "\n",
    "```\n",
    "# Read in the manually edited sequence database\n",
    "df = pd.read_csv(\"09_seq-database.csv\")\n",
    "\n",
    "# Write out a fasta file. \n",
    "topiary.write_fasta(df,\"10_new-alignment.fasta\",seq_name=\"pretty\",seq_column=\"alignment\")\n",
    "\n",
    "### edit in alivew and save as 11_aligned-edited.fasta\n",
    "\n",
    "# Load the alignment into the data frame\n",
    "df = topiary.load_fasta(df,\"11_aligned-edited.fasta\",load_into_column=\"alignment\")\n",
    "\n",
    "# Write out file\n",
    "df.to_csv(\"12_seq-database.csv\",index=False)\n",
    "\n",
    "```\n",
    "\n",
    "Copy `12_seq-database.csv` to your favorite cluster and use it to calculate a new maximum likelihood tree using an edited `run_raxml.srun` script that looks like this:\n",
    "\n",
    "```\n",
    "#!/bin/bash -l\n",
    "#SBATCH --account=harmslab      ### change this to your actual account for charging\n",
    "#SBATCH --job-name=raxml        ### job name\n",
    "#SBATCH --output=hostname.out   ### file in which to store job stdout\n",
    "#SBATCH --error=hostname.err    ### file in which to store job stderr\n",
    "#SBATCH --partition=long        ### can be short long fat longfat\n",
    "#SBATCH --time=07-00:00:00      ### Run for 7 days\n",
    "#SBATCH --nodes=1               ### Run on a single node\n",
    "#SBATCH --ntasks-per-node=1     ### Run one job on the node\n",
    "#SBATCH --cpus-per-task=28      ### Use 28 cores to run job (should match threads below)\n",
    "\n",
    "module load gcc\n",
    "\n",
    "# Find the best phylogenetic model\n",
    "# run-raxml model -c 07_seq-database.csv -o find-model -T 28\n",
    "\n",
    "# Consruct the ML tree\n",
    "run-raxml ml -c 12_seq-database.csv -m `cat find-model/best-model.txt` -o 13_ml-tree -T 28\n",
    "\n",
    "# Construct ancestors on the ML tree.\n",
    "run-raxml anc -c 12_seq-database.csv -m `cat find-model/best-model.txt` -t 13_ml-tree/02_ml-tree.newick -o 14_ml-anc -T 28\n",
    "\n",
    "# Construct ancestors on the reconciled generax tree with supports\n",
    "# run-raxml anc -c topiary.csv -m `cat find-model/best-model.txt` -t generax-run/reconciled-tree.newick -o ml-anc_reconciled --anc-support-tree generax-run/reconciled-tree-with-supports.newick -T 28\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Assign paralogs\n",
    "\n",
    "#### KO note: Not sure if we have to do this...\n",
    "\n",
    "Once you have a tree and alignment that you are happy with, you now need to identify which sequence corresponds to which paralog. The reverse BLAST protocol we used above is error-prone, particularly for highly diverged sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"07_seq-database.csv\") # or \"12_seq-database.csv\"\n",
    "\n",
    "df.loc[df.accession == \"OCT56128.1\",\"paralog\"] = \"LY96a\"\n",
    "df.loc[df.accession == \"XP_018120758.1\",\"paralog\"] = \"LY96b\"\n",
    "df.loc[df.accession == \"XP_040288185.1\",\"paralog\"] = \"LY86b\"\n",
    "df.loc[df.accession == \"XP_029446462.1\",\"paralog\"] = \"LY86b\"\n",
    "df.loc[df.accession == \"XP_029447354.1\",\"paralog\"] = \"LY86a\"\n",
    "df.loc[df.accession == \"XP_029446462.1\",\"paralog\"] = \"LY86b\"\n",
    "df.loc[df.accession == \"CAF5201687.1\",\"paralog\"] = \"LY96a\"\n",
    "df.loc[df.accession == \"XP_033791070.1\",\"paralog\"] = \"LY96a\"\n",
    "df.loc[df.accession == \"XP_040288186.1\",\"paralog\"] = \"LY86a\"\n",
    "\n",
    "df.to_csv(\"15_seq-database.csv\")\n",
    "\n",
    "\n",
    "# Make the sequence names on the output tree human readable. \n",
    "#topiary.util.uid_to_pretty(\"13_ml-tree/final-tree.newick\",\"15_ml-tree.newick\",df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load up `15_ml-tree.newick` in FigTree. Look for well-supported clades that contain sequences with known paralogy and make sure all of the sequences in that clade have the same paralog name.  For example, if you have a clade with SH = 100 that contains human S100A9, but also a pangolin protein labeled S100A8, the pangolin protein is labeled incorrectly. \n",
    "\n",
    "Either via pandas or excel, edit the `paralog` column of your sequence database with the correct call for each ortholog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generate species tree(s) and evaluate the maximum likelihood tree with bootstrapping\n",
    "\n",
    "Open a command or terminal window and log into your cluster account. In your home directory, start an interactive job. Example:\n",
    "```\n",
    "    srun --account=mylab --pty bash\n",
    "```\n",
    "\n",
    "Download the most recent version of topiary from the Harms Lab GitHub and prepare the topiary directory for installing python packages:\n",
    "\n",
    "```\n",
    "git clone https://github.com/harmslab/topiary.git\n",
    "cd topiary\n",
    "python setup.py install\n",
    "```\n",
    "cd into your working directory containing \"07_seq-database.csv\" file (or most recent version) and use the setup-generax script to create a directory called \"generax-trees\" containing bootstrap directories and an \"ml\" directory containing a newly made species tree, your alignment, and information about the maximum likelihood tree.\n",
    "\n",
    "```\n",
    "setup-generax -c CSV -t TREE -m MODEL -b BS_DIR -o OUTPUT\n",
    "Example: \n",
    "    setup-generax -c 07_seq-database.csv -t ml-tree/01_make-ml-tree/alignment.raxml.bestTree -m `cat find-model/best-model.txt` -b ml-tree/01_make-ml-tree/ -o generax-trees)\n",
    "```\n",
    "cd into the OUTPUT directory you just made (should be called generax-trees) and launch the generax bootstrapping script. This is very computationally heavy.\n",
    "```\n",
    "    bash 00_launch_generax_bootstrap.sh 20  \n",
    "```\n",
    "   Note: changing \"20\" to \"1\" will make this step run faster because you will be allocating 1 job to 1 node, instead of 20 jobs to 1 node. You may also want to edit the `00_launch_generax_bootstrap.sh` file to run for longer than 1 day. For large alignments (~1000 sequences) allocate 2 weeks time (\"long\" run). You might even use more than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Assemble bootstraps\n",
    "When it's all done running, in the \"generax-trees\" directory run the generax bootstrap assemble script:\n",
    "```\n",
    "    bash 01_assemble_generax_bootstrap.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Reconstruct ancestors on tree with supports\n",
    "\n",
    "If all of that goes smoothly, then go back to your original working directory that contains all of your files as well as ml and generax-trees directories.\n",
    "\n",
    "Reconstruct ancestors using the `run_raxml.srun` script by removing the comment from the very last line (after # Construct ancestors on the reconciled generax tree with supports) and commenting out the previous raxml lines. Make sure that this last line of code points to each of the modified files you wish to include in the reconstruction. Your .srun file should look like this:\n",
    "```\n",
    "#!/bin/bash -l\n",
    "#SBATCH --account=harmslab      ### change this to your actual account for charging\n",
    "#SBATCH --job-name=raxml        ### job name\n",
    "#SBATCH --output=hostname.out   ### file in which to store job stdout\n",
    "#SBATCH --error=hostname.err    ### file in which to store job stderr\n",
    "#SBATCH --partition=long        ### can be short long fat longfat\n",
    "#SBATCH --time=07-00:00:00      ### Run for 7 days\n",
    "#SBATCH --nodes=1               ### Run on a single node\n",
    "#SBATCH --ntasks-per-node=1     ### Run one job on the node\n",
    "#SBATCH --cpus-per-task=28      ### Use 28 cores to run job (should match threads below)\n",
    "\n",
    "module load gcc\n",
    "\n",
    "# Find the best phylogenetic model\n",
    "# run-raxml model -c 07_seq-database.csv -o find-model -T 28\n",
    "\n",
    "# Consruct the ML tree\n",
    "# run-raxml ml -c 07_seq-database.csv -m `cat find-model/best-model.txt` -o ml-tree -T 28\n",
    "\n",
    "# Construct ancestors on the ML tree.\n",
    "# run-raxml anc -c 07_seq-database.csv -m `cat find-model/best-model.txt` -t ml-tree/02_ml-tree.newick -o ml-anc -T 28\n",
    "\n",
    "# Construct ancestors on the reconciled generax tree with supports\n",
    "run-raxml anc -c topiary.csv -m `cat find-model/best-model.txt` -t generax-run/reconciled-tree.newick -o ml-anc_reconciled --anc-support-tree generax-run/reconciled-tree-with-supports.newick -T 28\n",
    "```\n",
    "Then, in your working directory, run your raxml script:\n",
    "```\n",
    "    qsub run_raxml.srun\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Evaluate ancestors\n",
    "Download your cluster working directory onto your computer for safe-keeping and further analysis.\n",
    "There should be a file called `10_ancestors_all.newick`. This is your phylogenetic tree constructed with information from the protein sequences you found and the known species tree. Open `10_ancestors_all.newick` in FigTree or your tree viewer of preference. Show the branch support values - branch support values indicate the degree to which one can be confident that the branch represents some \"signal\" present in the data. In other words, these values indicate how many times out of 100 (100/100 = 1) the same branch was observed when repeating the phylogenetic reconstruction on a re-sampled set of the data during the bootstrap analysis. Each reconstructed ancestral sequence is associated with a branch support value. We use these values to evaluate how confident we are in the reconstructed sequence. \n",
    "\n",
    "Furthermore, we can evaluate the ambiguity of our reconstructed ancestral sequences. Inside of the `ml-anc/02_final-ancestors/` directory you will find ancNodeXXX.pdf files. Opening one of these files shows a graph with posterior probability on the y-axis and alignment site on the x-axis. The black dots show the posterior probability that the amino acid called in the ancestor is the correct amino acid. The red dots show the posterior probability of the next most probable amino acid that could have been called at that the same position. The dashed line represents the posterior probability value if the site showed equal preference for any amino acid (ambiguous site), suggesting that red dots above this value represent amino acid substitutions that occurred frequently while red dots below this line suggest that the site's preference is skewed towards a specific amino acid. The histogram to the right displays the distribution of where these black and red points lie in the range of posterior probability. We have high confidence in our reconstructed ancestral node if the black and red histograms have little to no overlap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
